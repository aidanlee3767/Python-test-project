# https://newsapi.org/sources
# https://newsapi.org/docs/client-libraries/python
# flake8: noqa: E501

import time
from typing import Optional

import pandas as pd
import requests

from simple_agent.agents.parse_url import parse_article_content

NEWS_API_KEY = "b01f1c03cbb44b31a1ad0fb98c6eb155"


def get_latest_news_json(
    num_articles: int = 10,
    category: Optional[str] = None,
    country: Optional[str] = "us",
    query: Optional[str] = None,
    language: str = "en",
):
    """
    NewsAPIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ JSON í˜•íƒœë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        num_articles (int): ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        category (str): ì¹´í…Œê³ ë¦¬ (technology, business, entertainment, health, science, sports)
        country (str): êµ­ê°€ ì½”ë“œ (us, kr, etc.)
        query (str): ê²€ìƒ‰ í‚¤ì›Œë“œ (e.g., "Seoul", "culture", "K-pop")
        language (str): ì–¸ì–´ ì½”ë“œ (en, ko, etc.)

    Returns:
        list: ê¸°ì‚¬ ëª©ë¡. ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not NEWS_API_KEY:
        print("âŒ ì—ëŸ¬: NewsAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    # queryê°€ ìˆìœ¼ë©´ /everything ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (ë” ìœ ì—°í•¨)
    if query:
        url = "https://newsapi.org/v2/everything"
        params = {
            "apiKey": NEWS_API_KEY,
            "q": query,
            "language": language,
            "pageSize": num_articles,
            "sortBy": "publishedAt",  # ìµœì‹ ìˆœ
        }
    else:
        # queryê°€ ì—†ìœ¼ë©´ /top-headlines ì‚¬ìš©
        url = "https://newsapi.org/v2/top-headlines"
        params = {
            "apiKey": NEWS_API_KEY,
            "pageSize": num_articles,
        }
        if category:
            params["category"] = category
        if country:
            params["country"] = country

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        # API ì—ëŸ¬ ì²´í¬
        if data.get("status") != "ok":
            print(f"âŒ API ì—ëŸ¬: {data.get('message', 'Unknown error')}")
            return []

        articles = data.get("articles", [])

        if not articles:
            print("ğŸ’¡ ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"âœ… {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return articles

    except requests.exceptions.RequestException as e:
        print(f"âŒ API ìš”ì²­ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []
    except Exception as e:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []


def get_latest_news_formatted(
    num_articles: int = 10,
    category: Optional[str] = None,
    country: Optional[str] = "us",
    query: Optional[str] = None,
    language: str = "en",
):
    """ë‰´ìŠ¤ë¥¼ í¬ë§·íŒ…ëœ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    articles = get_latest_news_json(num_articles, category, country, query, language)

    formatted_news = []
    for i, article in enumerate(articles):
        title = article.get("title", "ì œëª© ì—†ìŒ")
        url = article.get("url", "")
        source = article.get("source", {}).get("name", "ì¶œì²˜ ë¶ˆëª…")
        formatted_news.append(f"{i + 1}. [{source}] {title}\n   - ë§í¬: {url}")

    return formatted_news


def get_latest_news_dataframe(
    num_articles: int = 10,
    category: Optional[str] = None,
    country: Optional[str] = "us",
    query: Optional[str] = None,
    language: str = "en",
    include_parsed_content: bool = True,
) -> pd.DataFrame:
    """ë‰´ìŠ¤ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    articles = get_latest_news_json(num_articles, category, country, query, language)

    if not articles:
        return pd.DataFrame()

    sources = []
    authors = []
    titles = []
    descriptions = []
    urls = []
    published_ats = []
    contents = []

    for article in articles:
        sources.append(article.get("source", {}).get("name", ""))
        authors.append(article.get("author", ""))
        titles.append(article.get("title", ""))
        descriptions.append(article.get("description", ""))
        urls.append(article.get("url", ""))
        published_ats.append(article.get("publishedAt", ""))
        contents.append(article.get("content", ""))

    new_article_data = {
        "Source": sources,
        "Author": authors,
        "Title": titles,
        "Description": descriptions,
        "URL": urls,
        "Published At": published_ats,
        "Content": contents,
    }

    df = pd.DataFrame(new_article_data)

    print("\nâœ… DataFrame ë³€í™˜ ì™„ë£Œ!")
    print(f"DataFrame í¬ê¸°: {df.shape} (í–‰: {df.shape[0]}, ì—´: {df.shape[1]})")

    if include_parsed_content:
        df = add_parsed_content_to_dataframe(df)
        print("\nâœ… íŒŒì‹± ì™„ë£Œ!")

    return df


def get_news_content(url: str):
    """ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        parsed_data = parse_article_content(url)

        res = {
            "Parsed Content": parsed_data.get("content", ""),
            "Parsed Author": parsed_data.get("author", ""),
            "Meta Description": parsed_data.get("meta_description", ""),
            "Parsed Publish Date": parsed_data.get("publish_date", ""),
            "Tags": ", ".join(parsed_data.get("tags", [])),
            "Word Count": parsed_data.get("word_count", 0),
            "Reading Time (min)": parsed_data.get("reading_time", 0),
            "Parse Status": parsed_data.get("status", "not_attempted"),
        }

        return res

    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return {
            "Parsed Content": "",
            "Parsed Author": "",
            "Meta Description": "",
            "Parsed Publish Date": "",
            "Tags": "",
            "Word Count": 0,
            "Reading Time (min)": 0,
            "Parse Status": "error",
        }


def add_parsed_content_to_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """ê¸°ì¡´ DataFrameì— URL íŒŒì‹± ê²°ê³¼ ì¶”ê°€"""
    df_copy = df.copy()

    # ìƒˆ ì»¬ëŸ¼ë“¤ ì´ˆê¸°í™”
    df_copy["Parsed Content"] = ""
    df_copy["Parsed Author"] = ""
    df_copy["Meta Description"] = ""
    df_copy["Parsed Publish Date"] = ""
    df_copy["Tags"] = ""
    df_copy["Word Count"] = 0
    df_copy["Reading Time (min)"] = 0
    df_copy["Parse Status"] = "pending"

    for index, row in df_copy.iterrows():
        print(f"ğŸ” [{index + 1}/{len(df_copy)}] {row['Title'][:40]}...")

        url = row["URL"]
        parsed_data = get_news_content(url)

        df_copy.at[index, "Parsed Content"] = parsed_data["Parsed Content"]
        df_copy.at[index, "Parsed Author"] = parsed_data["Parsed Author"]
        df_copy.at[index, "Meta Description"] = parsed_data["Meta Description"]
        df_copy.at[index, "Parsed Publish Date"] = parsed_data["Parsed Publish Date"]
        df_copy.at[index, "Tags"] = parsed_data["Tags"]
        df_copy.at[index, "Word Count"] = parsed_data["Word Count"]
        df_copy.at[index, "Reading Time (min)"] = parsed_data["Reading Time (min)"]
        df_copy.at[index, "Parse Status"] = parsed_data["Parse Status"]

        time.sleep(1)

    return df_copy


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ:

    # 1. IT ë‰´ìŠ¤ (ê¸°ì¡´ ë°©ì‹)
    print("\n=== IT ë‰´ìŠ¤ ===")
    df_tech = get_latest_news_dataframe(
        5, category="technology", include_parsed_content=False
    )

    # 2. Seoul ê´€ë ¨ ë‰´ìŠ¤
    print("\n=== Seoul ë‰´ìŠ¤ ===")
    df_seoul = get_latest_news_dataframe(5, query="Seoul", include_parsed_content=False)

    # 3. ë¬¸í™” ë‰´ìŠ¤
    print("\n=== ë¬¸í™” ë‰´ìŠ¤ ===")
    df_culture = get_latest_news_dataframe(
        5, query="culture", include_parsed_content=False
    )

    # 4. í•œêµ­ ë‰´ìŠ¤ (í•œê¸€)
    print("\n=== í•œêµ­ ë‰´ìŠ¤ ===")
    df_korea = get_latest_news_dataframe(
        5, country="kr", language="ko", include_parsed_content=False
    )

    # CSV ì €ì¥ ì˜ˆì‹œ
    df_tech.to_csv("it_news_articles.csv", index=False)
